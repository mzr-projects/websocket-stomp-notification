version: '3.8'

services:
  jobmanager:
    image: flink:1.18.0-scala_2.12-java11
    hostname: jobmanager
    container_name: flink-jobmanager
    network_mode: host
    environment:
      - FLINK_PROPERTIES=
        jobmanager.rpc.address: localhost
        jobmanager.rpc.port: 6123
        jobmanager.memory.process.size: 1600m
        jobmanager.execution.failover-strategy: region
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 2
        taskmanager.memory.process.size: 1728m
        taskmanager.memory.flink.size: 1280m
        # Checkpoint configuration
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
        execution.checkpointing.interval: 60000
        execution.checkpointing.min-pause: 5000
        execution.checkpointing.timeout: 600000
        execution.checkpointing.max-concurrent-checkpoints: 1
        # High availability configuration
        high-availability: zookeeper
        high-availability.zookeeper.quorum: localhost:2181
        high-availability.storageDir: file:///tmp/flink-ha
        high-availability.cluster-id: /flink-cluster
        # Security and monitoring
        metrics.reporters: prom
        metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
        metrics.reporter.prom.host: localhost
        metrics.reporter.prom.port: 9249
        # Network buffer configuration
        taskmanager.network.memory.fraction: 0.1
        taskmanager.network.memory.min: 64mb
        taskmanager.network.memory.max: 1gb
    volumes:
      - flink_data:/tmp/flink-checkpoints
      - flink_savepoints:/tmp/flink-savepoints
      - flink_ha:/tmp/flink-ha
      - ./flink-conf:/opt/flink/conf
      - ./jobs:/opt/flink/jobs
      - ./logs:/opt/flink/log
    command: jobmanager
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  taskmanager:
    image: flink:1.18.0-scala_2.12-java11
    hostname: taskmanager
    container_name: flink-taskmanager
    network_mode: host
    environment:
      - FLINK_PROPERTIES=
        jobmanager.rpc.address: localhost
        jobmanager.rpc.port: 6123
        taskmanager.rpc.port: 6122
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 2
        taskmanager.memory.process.size: 1728m
        taskmanager.memory.flink.size: 1280m
        # Network configuration
        taskmanager.host: localhost
        taskmanager.bind-host: 0.0.0.0
        taskmanager.data.port: 6121
        # State backend configuration
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
        # High availability configuration
        high-availability: zookeeper
        high-availability.zookeeper.quorum: localhost:2181
        high-availability.storageDir: file:///tmp/flink-ha
        high-availability.cluster-id: /flink-cluster
        # Performance tuning
        taskmanager.network.memory.fraction: 0.1
        taskmanager.network.memory.min: 64mb
        taskmanager.network.memory.max: 1gb
        taskmanager.network.netty.num-arenas: 16
    volumes:
      - flink_data:/tmp/flink-checkpoints
      - flink_savepoints:/tmp/flink-savepoints
      - flink_ha:/tmp/flink-ha
      - ./flink-conf:/opt/flink/conf
      - ./jobs:/opt/flink/jobs
      - ./logs:/opt/flink/log
    command: taskmanager
    restart: unless-stopped
    depends_on:
      - jobmanager
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "netstat -tulpn | grep :6121 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Optional: Additional TaskManager for scaling
  taskmanager-2:
    image: flink:1.18.0-scala_2.12-java11
    hostname: taskmanager-2
    container_name: flink-taskmanager-2
    network_mode: host
    environment:
      - FLINK_PROPERTIES=
        jobmanager.rpc.address: localhost
        jobmanager.rpc.port: 6123
        taskmanager.rpc.port: 6124
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 2
        taskmanager.memory.process.size: 1728m
        taskmanager.memory.flink.size: 1280m
        taskmanager.host: localhost
        taskmanager.bind-host: 0.0.0.0
        taskmanager.data.port: 6125
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
        high-availability: zookeeper
        high-availability.zookeeper.quorum: localhost:2181
        high-availability.storageDir: file:///tmp/flink-ha
        high-availability.cluster-id: /flink-cluster
        taskmanager.network.memory.fraction: 0.1
        taskmanager.network.memory.min: 64mb
        taskmanager.network.memory.max: 1gb
        taskmanager.network.netty.num-arenas: 16
    volumes:
      - flink_data:/tmp/flink-checkpoints
      - flink_savepoints:/tmp/flink-savepoints
      - flink_ha:/tmp/flink-ha
      - ./flink-conf:/opt/flink/conf
      - ./jobs:/opt/flink/jobs
      - ./logs:/opt/flink/log
    command: taskmanager
    restart: unless-stopped
    depends_on:
      - jobmanager
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "3"
    profiles:
      - scaling
    healthcheck:
      test: ["CMD-SHELL", "netstat -tulpn | grep :6125 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Zookeeper for High Availability
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: flink-zookeeper
    network_mode: host
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_LOG4J_LOGGERS: "org.apache.zookeeper=WARN"
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: "WARN"
    volumes:
      - zk_data:/var/lib/zookeeper/data
      - zk_logs:/var/lib/zookeeper/log
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Optional: Prometheus for monitoring
  prometheus:
    image: prom/prometheus:v2.45.0
    hostname: prometheus
    container_name: flink-prometheus
    network_mode: host
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    profiles:
      - monitoring
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

volumes:
  flink_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/checkpoints
  flink_savepoints:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/savepoints
  flink_ha:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/ha
  zk_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/zk-data
  zk_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/zk-logs
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/prometheus